# Required Libraries
if (!require(pacman)) install.packages("pacman")
pacman::p_load(
forecast, tseries, ggplot2, dplyr, lubridate, readr,
VIM, mice, corrplot, plotly, gridExtra, psych,
zoo, imputeTS, outliers, robustbase
)
cat("Loading data files...\n")
weather <- read_csv("weatherData.csv")
# Required Libraries
if (!require(pacman)) install.packages("pacman")
pacman::p_load(
forecast, tseries, ggplot2, dplyr, lubridate, readr,
VIM, mice, corrplot, plotly, gridExtra, psych,
zoo, imputeTS, outliers, robustbase
)
cat("Loading data files...\n")
weather <- read_csv("weatherData.csv")
locations <- read_csv("locationData.csv")
# Display basic information about the datasets
cat("\n=== INITIAL DATA INSPECTION ===\n")
cat("Weather data dimensions:", dim(weather), "\n")
cat("Location data dimensions:", dim(locations), "\n")
# Display structure and column names
str(weather)
str(locations)
# Check for encoding issues in column names
cat("\nOriginal column names:\n")
print(names(weather))
# Fix encoding issues in column names
names(weather) <- gsub("Â", "", names(weather))
names(weather) <- gsub("°", "", names(weather))
names(weather) <- gsub("\\s+", "_", names(weather))  # Replace spaces with underscores
cat("\nCleaned column names:\n")
print(names(weather))
cat("\n=== DATA PREPROCESSING ===\n")
# 2.1 Date Processing
cat("Processing date column...\n")
weather <- weather %>%
mutate(
date = as.Date(date, format = "%m/%d/%Y"),
year = year(date),
month = month(date),
day = day(date),
quarter = quarter(date),
week = week(date),
day_of_year = yday(date)
) %>%
left_join(locations %>% select(location_id, city_name), by = "location_id")
# Check date range
cat("Date range:", as.character(range(weather$date, na.rm = TRUE)), "\n")
# 2.2 Missing Values Analysis
cat("\nAnalyzing missing values...\n")
# Calculate missing value percentages
missing_summary <- weather %>%
summarise_all(~sum(is.na(.))) %>%
gather(key = "variable", value = "missing_count") %>%
mutate(missing_percentage = round((missing_count / nrow(weather)) * 100, 2)) %>%
arrange(desc(missing_percentage))
print(missing_summary)
# Visualize missing values pattern
png("Missing_Values_Pattern.png", width = 1200, height = 800)
VIM::aggr(weather, col = c('navyblue','red'), numbers = TRUE, sortVars = TRUE)
title("Missing Values Pattern")
dev.off()
# 2.3 Outlier Detection and Treatment
cat("\nDetecting outliers...\n")
# Function to detect outliers using IQR method
detect_outliers_iqr <- function(x) {
Q1 <- quantile(x, 0.25, na.rm = TRUE)
Q3 <- quantile(x, 0.75, na.rm = TRUE)
IQR <- Q3 - Q1
lower_bound <- Q1 - 1.5 * IQR
upper_bound <- Q3 + 1.5 * IQR
return(which(x < lower_bound | x > upper_bound))
}
# Identify numeric columns (excluding ID and date columns)
numeric_cols <- weather %>%
select_if(is.numeric) %>%
select(-location_id, -year, -month, -day, -quarter, -week, -day_of_year) %>%
names()
# Create outlier summary
outlier_summary <- data.frame(
Variable = character(),
Outlier_Count = integer(),
Outlier_Percentage = numeric(),
stringsAsFactors = FALSE
)
for (col in numeric_cols) {
if (col %in% names(weather)) {
outliers <- detect_outliers_iqr(weather[[col]])
outlier_summary <- rbind(outlier_summary, data.frame(
Variable = col,
Outlier_Count = length(outliers),
Outlier_Percentage = round((length(outliers) / nrow(weather)) * 100, 2)
))
}
}
print(outlier_summary)
# 2.4 Data Cleaning and Imputation
cat("\nCleaning and imputing data...\n")
weather_clean <- weather
# Handle missing values - use time series imputation
for (col in numeric_cols) {
if (col %in% names(weather_clean) && sum(is.na(weather_clean[[col]])) > 0) {
cat("Imputing missing values for", col, "...\n")
weather_clean[[col]] <- na_interpolation(weather_clean[[col]], option = "spline")
}
}
# Handle extreme outliers (cap at 99th and 1st percentiles)
for (col in numeric_cols) {
if (col %in% names(weather_clean)) {
p01 <- quantile(weather_clean[[col]], 0.01, na.rm = TRUE)
p99 <- quantile(weather_clean[[col]], 0.99, na.rm = TRUE)
weather_clean[[col]] <- pmax(pmin(weather_clean[[col]], p99), p01)
}
}
# Create monthly aggregated data
weather_monthly <- weather_clean %>%
mutate(month_year = floor_date(date, "month")) %>%
group_by(location_id, city_name, month_year) %>%
summarise(
temperature_mean = mean(get(grep("temperature.*mean", names(.), value = TRUE)[1]), na.rm = TRUE),
precipitation_sum = sum(get(grep("precipitation.*sum", names(.), value = TRUE)[1]), na.rm = TRUE),
humidity_mean = if(any(grepl("humidity", names(.)))) mean(get(grep("humidity", names(.), value = TRUE)[1]), na.rm = TRUE) else NA,
wind_speed_mean = if(any(grepl("wind", names(.)))) mean(get(grep("wind", names(.), value = TRUE)[1]), na.rm = TRUE) else NA,
.groups = 'drop'
) %>%
arrange(city_name, month_year)
cat("\n=== EXPLORATORY DATA ANALYSIS ===\n")
# Create output directories
dir.create("EDA_Plots", showWarnings = FALSE)
dir.create("Summary_Statistics", showWarnings = FALSE)
# 3.1 Descriptive Statistics
cat("Generating descriptive statistics...\n")
desc_stats <- weather_monthly %>%
select_if(is.numeric) %>%
select(-location_id) %>%
describe()
write.csv(desc_stats, "Summary_Statistics/Descriptive_Statistics.csv")
# 3.2 Time Series Characteristics by District
districts <- unique(weather_monthly$city_name)
cat("Number of districts:", length(districts), "\n")
# Initialize summary data frame
ts_characteristics <- data.frame()
for (district in districts[!is.na(districts)]) {
cat("Analyzing", district, "...\n")
district_data <- weather_monthly %>%
filter(city_name == district) %>%
arrange(month_year)
if (nrow(district_data) < 24) {
cat("Insufficient data for", district, "- skipping\n")
next
}
# Create time series objects
ts_start <- c(year(min(district_data$month_year)), month(min(district_data$month_year)))
temp_ts <- ts(district_data$temperature_mean, start = ts_start, frequency = 12)
rain_ts <- ts(district_data$precipitation_sum, start = ts_start, frequency = 12)
# 3.3 Time Series Plots
png(paste0("EDA_Plots/", district, "_Temperature_TimeSeries.png"), width = 1200, height = 600)
plot(temp_ts, main = paste("Temperature Time Series -", district),
xlab = "Year", ylab = "Temperature (°C)", col = "red", lwd = 2)
grid()
dev.off()
png(paste0("EDA_Plots/", district, "_Rainfall_TimeSeries.png"), width = 1200, height = 600)
plot(rain_ts, main = paste("Rainfall Time Series -", district),
xlab = "Year", ylab = "Rainfall (mm)", col = "blue", lwd = 2)
grid()
dev.off()
# 3.4 Seasonal Decomposition
if (length(temp_ts) >= 24) {
decomp_temp <- stl(temp_ts, s.window = "periodic")
decomp_rain <- stl(rain_ts, s.window = "periodic")
png(paste0("EDA_Plots/", district, "_Temperature_Decomposition.png"), width = 1200, height = 800)
plot(decomp_temp, main = paste("Temperature Decomposition -", district))
dev.off()
png(paste0("EDA_Plots/", district, "_Rainfall_Decomposition.png"), width = 1200, height = 800)
plot(decomp_rain, main = paste("Rainfall Decomposition -", district))
dev.off()
}
# 3.5 Autocorrelation Analysis
png(paste0("EDA_Plots/", district, "_ACF_PACF.png"), width = 1200, height = 800)
par(mfrow = c(2, 2))
acf(temp_ts, main = paste("ACF - Temperature -", district))
pacf(temp_ts, main = paste("PACF - Temperature -", district))
acf(rain_ts, main = paste("ACF - Rainfall -", district))
pacf(rain_ts, main = paste("PACF - Rainfall -", district))
dev.off()
# 3.6 Stationarity Tests
adf_temp <- adf.test(temp_ts, alternative = "stationary")
pp_temp <- pp.test(temp_ts, alternative = "stationary")
kpss_temp <- kpss.test(temp_ts, null = "Level")
adf_rain <- adf.test(rain_ts, alternative = "stationary")
pp_rain <- pp.test(rain_ts, alternative = "stationary")
kpss_rain <- kpss.test(rain_ts, null = "Level")
# 3.7 Seasonal Patterns Analysis
seasonal_temp <- aggregate(temp_ts, nfrequency = 12, FUN = mean)
seasonal_rain <- aggregate(rain_ts, nfrequency = 12, FUN = mean)
# Store characteristics
ts_char <- data.frame(
District = district,
Observations = length(temp_ts),
Temp_Mean = round(mean(temp_ts, na.rm = TRUE), 2),
Temp_SD = round(sd(temp_ts, na.rm = TRUE), 2),
Temp_Min = round(min(temp_ts, na.rm = TRUE), 2),
Temp_Max = round(max(temp_ts, na.rm = TRUE), 2),
Rain_Mean = round(mean(rain_ts, na.rm = TRUE), 2),
Rain_SD = round(sd(rain_ts, na.rm = TRUE), 2),
Rain_Min = round(min(rain_ts, na.rm = TRUE), 2),
Rain_Max = round(max(rain_ts, na.rm = TRUE), 2),
ADF_Temp_pvalue = round(adf_temp$p.value, 4),
ADF_Rain_pvalue = round(adf_rain$p.value, 4),
PP_Temp_pvalue = round(pp_temp$p.value, 4),
PP_Rain_pvalue = round(pp_rain$p.value, 4),
KPSS_Temp_pvalue = round(kpss_temp$p.value, 4),
KPSS_Rain_pvalue = round(kpss_rain$p.value, 4)
)
ts_characteristics <- rbind(ts_characteristics, ts_char)
}
# Save time series characteristics
write.csv(ts_characteristics, "Summary_Statistics/Time_Series_Characteristics.csv", row.names = FALSE)
# 3.8 Overall Correlation Analysis
cat("Generating correlation analysis...\n")
correlation_data <- weather_monthly %>%
select_if(is.numeric) %>%
select(-location_id) %>%
cor(use = "complete.obs")
View(kpss_temp)
png("EDA_Plots/Correlation_Matrix.png", width = 800, height = 800)
corrplot(correlation_data, method = "circle", type = "upper",
order = "hclust", tl.cex = 0.8, tl.col = "black")
title("Correlation Matrix - Weather Variables")
dev.off()
# 3.9 Monthly Seasonal Patterns (All Districts Combined)
monthly_patterns <- weather_monthly %>%
mutate(month = month(month_year, label = TRUE)) %>%
group_by(month) %>%
summarise(
avg_temp = mean(temperature_mean, na.rm = TRUE),
avg_rainfall = mean(precipitation_sum, na.rm = TRUE),
.groups = 'drop'
)
png("EDA_Plots/Monthly_Seasonal_Patterns.png", width = 1200, height = 600)
par(mfrow = c(1, 2))
barplot(monthly_patterns$avg_temp, names.arg = monthly_patterns$month,
main = "Average Monthly Temperature", ylab = "Temperature (°C)",
col = "red", las = 2)
barplot(monthly_patterns$avg_rainfall, names.arg = monthly_patterns$month,
main = "Average Monthly Rainfall", ylab = "Rainfall (mm)",
col = "blue", las = 2)
dev.off()
cat("\n=== DATA QUALITY ASSESSMENT ===\n")
# Create comprehensive data quality report
quality_report <- list(
"Data Dimensions" = dim(weather_clean),
"Date Range" = range(weather_clean$date, na.rm = TRUE),
"Districts Analyzed" = length(unique(weather_clean$city_name)),
"Missing Values (Original)" = colSums(is.na(weather)),
"Missing Values (After Cleaning)" = colSums(is.na(weather_clean)),
"Outliers Detected" = outlier_summary,
"Stationarity Tests" = ts_characteristics[, c("District", "ADF_Temp_pvalue", "ADF_Rain_pvalue")]
)
# Save quality report
capture.output(quality_report, file = "Summary_Statistics/Data_Quality_Report.txt")
cat("\n=== PREPROCESSING AND EDA COMPLETE ===\n")
cat("Files generated:\n")
cat("- Missing_Values_Pattern.png\n")
cat("- EDA_Plots/ (directory with all exploratory plots)\n")
cat("- Summary_Statistics/ (directory with statistical summaries)\n")
cat("\nCleaned data objects created:\n")
cat("- weather_clean: Daily cleaned data\n")
cat("- weather_monthly: Monthly aggregated data\n")
cat("- ts_characteristics: Time series properties by district\n")
cat("\nReady for SARIMA model fitting!\n")
cat("\n=== SARIMA MODEL FITTING AND FORECASTING ===\n")
# Create directories to store forecast plots
dir.create("Forecast_Plots", showWarnings = FALSE)
dir.create("Forecast_Results", showWarnings = FALSE)
library(forecast)
# Function to fit SARIMA and forecast
forecast_district <- function(district_name, h = 24) {
cat("Processing:", district_name, "\n")
district_data <- weather_monthly %>%
filter(city_name == district_name) %>%
arrange(month_year)
if (nrow(district_data) < 36) {
cat("Not enough data for:", district_name, "\n")
return(NULL)
}
ts_start <- c(year(min(district_data$month_year)), month(min(district_data$month_year)))
temp_ts <- ts(district_data$temperature_mean, start = ts_start, frequency = 12)
rain_ts <- ts(district_data$precipitation_sum, start = ts_start, frequency = 12)
# Fit SARIMA models using auto.arima
temp_model <- auto.arima(temp_ts, seasonal = TRUE, stepwise = FALSE, approximation = FALSE)
rain_model <- auto.arima(rain_ts, seasonal = TRUE, stepwise = FALSE, approximation = FALSE)
# Forecast
temp_forecast <- forecast(temp_model, h = h)
rain_forecast <- forecast(rain_model, h = h)
# Plot and save
png(paste0("Forecast_Plots/", district_name, "_Temp_Forecast.png"), width = 1200, height = 600)
plot(temp_forecast, main = paste("Temperature Forecast -", district_name),
ylab = "Temperature (°C)", xlab = "Year")
grid()
dev.off()
png(paste0("Forecast_Plots/", district_name, "_Rain_Forecast.png"), width = 1200, height = 600)
plot(rain_forecast, main = paste("Rainfall Forecast -", district_name),
ylab = "Rainfall (mm)", xlab = "Year")
grid()
dev.off()
# Return results
return(list(
district = district_name,
temp_model = temp_model,
rain_model = rain_model,
temp_forecast = temp_forecast,
rain_forecast = rain_forecast
))
}
# Apply forecasting function to all districts
forecast_results <- list()
for (district in unique(weather_monthly$city_name)) {
result <- forecast_district(district, h = 24)
if (!is.null(result)) {
forecast_results[[district]] <- result
# Save forecast data
forecast_df <- data.frame(
Date = seq(from = max(weather_monthly$month_year) + months(1), by = "month", length.out = 24),
Temp_Forecast = as.numeric(result$temp_forecast$mean),
Temp_Lower80 = as.numeric(result$temp_forecast$lower[,1]),
Temp_Upper80 = as.numeric(result$temp_forecast$upper[,1]),
Rain_Forecast = as.numeric(result$rain_forecast$mean),
Rain_Lower80 = as.numeric(result$rain_forecast$lower[,1]),
Rain_Upper80 = as.numeric(result$rain_forecast$upper[,1])
)
write.csv(forecast_df, paste0("Forecast_Results/", district, "_Forecast.csv"), row.names = FALSE)
}
}
cat("\n=== SARIMA Forecasting Complete ===\n")
cat("Forecasts and plots saved in 'Forecast_Plots/' and 'Forecast_Results/' directories.\n")
summary(temp_ts)
View(adf_rain)
adf_rain[["statistic"]]
View(adf_temp)
adf_temp[["statistic"]][["Dickey-Fuller"]]
View(weather_monthly)
View(decomp_rain)
decomp_rain[["call"]]
summary( temp_model)
summary(forecast_results[["Colombo"]]$temp_model)
summary(forecast_results[["Colombo"]]$rain_model)
sink("Summary_Statistics/SARIMA_Model_Summaries.txt")
for (district in names(forecast_results)) {
cat("\n==== District:", district, "====\n")
cat("\n--- Temperature Model Summary ---\n")
print(summary(forecast_results[[district]]$temp_model))
cat("\n--- Rainfall Model Summary ---\n")
print(summary(forecast_results[[district]]$rain_model))
}
sink()  # stop writing to file
sink("Summary_Statistics/SARIMA_Model_Summaries.txt")
for (district in names(forecast_results)) {
cat("\n==== District:", district, "====\n")
cat("\n--- Temperature Model Summary ---\n")
print(summary(forecast_results[[district]]$temp_model))
cat("\n--- Rainfall Model Summary ---\n")
print(summary(forecast_results[[district]]$rain_model))
}
sink("Summary_Statistics/SARIMA_Model_Summaries-2.txt")
for (district in names(forecast_results)) {
cat("\n==== District:", district, "====\n")
cat("\n--- Temperature Model Summary ---\n")
print(summary(forecast_results[[district]]$temp_model))
cat("\n--- Rainfall Model Summary ---\n")
print(summary(forecast_results[[district]]$rain_model))
}
# Residual Diagnostics for Temperature Model
png(paste0("Forecast_Plots/", district_name, "_Temp_Residuals.png"), width = 1200, height = 800)
checkresiduals(temp_model)
# Fit SARIMA models using auto.arima
temp_model <- auto.arima(temp_ts, seasonal = TRUE, stepwise = FALSE, approximation = FALSE)
rain_model <- auto.arima(rain_ts, seasonal = TRUE, stepwise = FALSE, approximation = FALSE)
# Residual Diagnostics for Temperature Model
png(paste0("Forecast_Plots/", district_name, "_Temp_Residuals.png"), width = 1200, height = 800)
